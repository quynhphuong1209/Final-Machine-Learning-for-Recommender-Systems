# -*- coding: utf-8 -*-
"""Qu·ª≥nh Ph∆∞∆°ng BaÃÄi_taÃ£ÃÇp_thuÃõÃ£c_haÃÄnh_keÃÇÃÅt_thuÃÅc_hoÃ£c_phaÃÇÃÄn (final).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tuEU1E_dYSBdmPq-KsyZcCyslTvujO7s

# C√ÅC B∆Ø·ªöC C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN
"""

# B∆∞·ªõc 1: G·ª° c√†i ƒë·∫∑t numpy v√† scikit-surprise ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng c√≥ phi√™n b·∫£n xung ƒë·ªôt
!pip uninstall numpy scikit-surprise -y

# B∆∞·ªõc 2: C√†i ƒë·∫∑t m·ªôt phi√™n b·∫£n numpy c·ª• th·ªÉ (1.x) v√† scikit-surprise
# numpy==1.26.6 l√† m·ªôt phi√™n b·∫£n ·ªïn ƒë·ªãnh thu·ªôc d√≤ng 1.x, t∆∞∆°ng th√≠ch v·ªõi surprise
!pip install numpy==1.26.4 scikit-surprise

# Sau khi ch·∫°y c√°c l·ªánh tr√™n, b·∫°n c·∫ßn KH·ªûI ƒê·ªòNG L·∫†I TH·ªúI GIAN CH·∫†Y (RUNTIME/KERNEL) c·ªßa Colab.
# B·∫°n c√≥ th·ªÉ l√†m ƒëi·ªÅu n√†y b·∫±ng c√°ch v√†o menu "Runtime" -> "Restart runtime" (ho·∫∑c "M√¥i tr∆∞·ªùng ch·∫°y" -> "Kh·ªüi ƒë·ªông l·∫°i m√¥i tr∆∞·ªùng ch·∫°y").
# CH·ªà SAU KHI KH·ªûI ƒê·ªòNG L·∫†I, H√ÉY CH·∫†Y C√ÅC CELL KH√ÅC C·ª¶A B·∫†N.

!pip install scikit-surprise

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# ...

"""# CH·∫∂NG 1: CHU·∫®N B·ªä D·ªÆ LI·ªÜU"""

# T·∫°o d·ªØ li·ªáu b·ªánh nh√¢n
def create_patient_data(n_patients=1000):
    np.random.seed(42)
    patient_ids = [f'P{i:04d}' for i in range(1, n_patients+1)]
    # Th√¥ng tin c∆° b·∫£n
    ages = np.random.normal(50, 15, n_patients).astype(int)
    ages = np.clip(ages, 18, 90)
    genders = np.random.choice(['Nam', 'N·ªØ'], size=n_patients)
    # B·ªánh l√Ω
    conditions = np.random.choice(
        ['Tim m·∫°ch', 'H√¥ h·∫•p', 'Ti√™u h√≥a', 'Th·∫ßn kinh', 'N·ªôi ti·∫øt',
         'Da li·ªÖu', 'C∆° x∆∞∆°ng kh·ªõp', 'Huy·∫øt h·ªçc', 'Th·∫≠n - Ti·∫øt ni·ªáu', 'Kh√°c'],
        size=n_patients
    )
    # Ti·ªÅn s·ª≠
    history_severity = np.random.choice(['Kh√¥ng', 'Nh·∫π', 'Trung b√¨nh', 'N·∫∑ng'], size=n_patients)
    # Ch·ªâ s·ªë s·ª©c kh·ªèe
    health_index = np.random.normal(70, 15, n_patients).astype(int)
    health_index = np.clip(health_index, 0, 100)
    # T·∫°o DataFrame
    patients = pd.DataFrame({
        'PatientID': patient_ids,
        'Age': ages,
        'Gender': genders,
        'Condition': conditions,
        'HistorySeverity': history_severity,
        'HealthIndex': health_index
    })
    return patients

# T·∫°o d·ªØ li·ªáu thu·ªëc
def create_drug_data(n_drugs=200):
    np.random.seed(43)
    drug_ids = [f'D{i:04d}' for i in range(1, n_drugs+1)]
    drug_types = np.random.choice(
        ['Kh√°ng sinh', 'Gi·∫£m ƒëau', 'H·∫° s·ªët', 'Tim m·∫°ch', 'Ti√™u h√≥a',
         'Th·∫ßn kinh', 'N·ªôi ti·∫øt', 'Vitamin', 'Ch·ªëng vi√™m', 'Kh√°c'],
        size=n_drugs
    )
    purposes = np.random.choice(
        ['ƒêi·ªÅu tr·ªã', 'Ph√≤ng ng·ª´a', 'Gi·∫£m tri·ªáu ch·ª©ng', 'H·ªó tr·ª£'],
        size=n_drugs
    )
    side_effects = np.random.choice(
        ['Kh√¥ng', 'Nh·∫π', 'Trung b√¨nh', 'N·∫∑ng'],
        size=n_drugs
    )
    contraindications = np.random.choice(
        ['Kh√¥ng', 'Mang thai', 'Tr·∫ª em', 'Ng∆∞·ªùi cao tu·ªïi', 'B·ªánh gan', 'B·ªánh th·∫≠n'],
        size=n_drugs
    )
    drugs = pd.DataFrame({
        'DrugID': drug_ids,
        'Type': drug_types,
        'Purpose': purposes,
        'SideEffect': side_effects,
        'Contraindication': contraindications
    })
    return drugs

# T·∫°o ma tr·∫≠n ƒë√°nh gi√°
def create_ratings_data(patients, drugs, density=0.05):
    np.random.seed(44)
    n_patients = len(patients)
    n_drugs = len(drugs)
    n_ratings = int(n_patients * n_drugs * density)
    patient_indices = np.random.choice(n_patients, n_ratings)
    drug_indices = np.random.choice(n_drugs, n_ratings)
    ratings = []
    for i in range(n_ratings):
        patient_idx = patient_indices[i]
        drug_idx = drug_indices[i]
        patient_condition = patients.iloc[patient_idx]['Condition']
        drug_type = drugs.iloc[drug_idx]['Type']
        if (patient_condition == 'Tim m·∫°ch' and drug_type == 'Tim m·∫°ch') or \
           (patient_condition == 'H√¥ h·∫•p' and drug_type == 'Kh√°ng sinh') or \
           (patient_condition == 'Ti√™u h√≥a' and drug_type == 'Ti√™u h√≥a') or \
           (patient_condition == 'Th·∫ßn kinh' and drug_type == 'Th·∫ßn kinh') or \
           (patient_condition == 'N·ªôi ti·∫øt' and drug_type == 'N·ªôi ti·∫øt'):
            rating = np.random.choice([3, 4, 5], p=[0.2, 0.3, 0.5])
        else:
            rating = np.random.choice([1, 2, 3, 4, 5])
        ratings.append(rating)
    ratings_df = pd.DataFrame({
        'PatientID': [patients.iloc[i]['PatientID'] for i in patient_indices],
        'DrugID': [drugs.iloc[i]['DrugID'] for i in drug_indices],
        'Rating': ratings,
        'Timestamp': np.random.randint(1577836800, 1640995200, n_ratings)
    })
    ratings_df = ratings_df.sort_values('Timestamp').drop_duplicates(
        subset=['PatientID', 'DrugID'], keep='last'
    )
    return ratings_df

# TODO: T·∫°o d·ªØ li·ªáu
patients = create_patient_data(1000)
drugs = create_drug_data(200)
ratings = create_ratings_data(patients, drugs, density=0.05)
# G·ªôp th√¥ng tin t·ª´ c√°c b·∫£ng
ratings_full = ratings.merge(patients, on='PatientID').merge(drugs, on='DrugID')
ratings_full.head()

# TODO: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
# 1. X·ª≠ l√Ω gi√° tr·ªã thi·∫øu v√† ngo·∫°i lai
# Ki·ªÉm tra v√† lo·∫°i b·ªè c√°c gi√° tr·ªã thi·∫øu
print("S·ªë l∆∞·ª£ng gi√° tr·ªã thi·∫øu tr∆∞·ªõc khi x·ª≠ l√Ω:")
print(ratings_full.isnull().sum())
ratings_full = ratings_full.dropna()

# Lo·∫°i b·ªè ngo·∫°i lai trong bi·∫øn s·ªë: v√≠ d·ª• HealthIndex n√™n n·∫±m trong kho·∫£ng 0‚Äì100
ratings_full = ratings_full[(ratings_full['HealthIndex'] >= 0) & (ratings_full['HealthIndex'] <= 100)]

# Ki·ªÉm tra l·∫°i sau khi x·ª≠ l√Ω
print("S·ªë l∆∞·ª£ng c√≤n l·∫°i sau khi x·ª≠ l√Ω thi·∫øu v√† ngo·∫°i lai:", len(ratings_full))

# 2. M√£ h√≥a bi·∫øn ph√¢n lo·∫°i (one-hot encoding)
# C√°c bi·∫øn ph√¢n lo·∫°i c·∫ßn ƒë∆∞·ª£c m√£ h√≥a
categorical_cols = ['Gender', 'Condition', 'HistorySeverity', 'Type', 'Purpose', 'SideEffect', 'Contraindication']

# √Åp d·ª•ng one-hot encoding cho c√°c c·ªôt ph√¢n lo·∫°i
ratings_encoded = pd.get_dummies(ratings_full, columns=categorical_cols)

# Ki·ªÉm tra k·∫øt qu·∫£
ratings_encoded.head()

# 3. Chu·∫©n h√≥a bi·∫øn s·ªë
from sklearn.preprocessing import StandardScaler

# C√°c c·ªôt s·ªë c·∫ßn chu·∫©n h√≥a
numeric_cols = ['Age', 'HealthIndex']

# Chu·∫©n h√≥a b·∫±ng StandardScaler
scaler = StandardScaler()
ratings_encoded[numeric_cols] = scaler.fit_transform(ratings_encoded[numeric_cols])

# Ki·ªÉm tra sau chu·∫©n h√≥a
ratings_encoded[numeric_cols].describe()

# TODO: Ph√¢n t√≠ch d·ªØ li·ªáu
# 1. Th·ªëng k√™ m√¥ t·∫£
# Th·ªëng k√™ m√¥ t·∫£ c√°c ƒë·∫∑c tr∆∞ng s·ªë trong d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
print("M√¥ t·∫£ th·ªëng k√™ c√°c bi·∫øn s·ªë:")
ratings_encoded.describe()

# 2. Ph√¢n t√≠ch ma tr·∫≠n ƒë√°nh gi√°
# T·∫°o ma tr·∫≠n ƒë√°nh gi√° b·ªánh nh√¢n - thu·ªëc (PatientID x DrugID)
ratings_matrix = ratings.pivot(index='PatientID', columns='DrugID', values='Rating')

# Ki·ªÉm tra m·ªôt ph·∫ßn ma tr·∫≠n
ratings_matrix.head()

# Th·ªëng k√™ s·ªë l∆∞·ª£ng ƒë√°nh gi√° m·ªói b·ªánh nh√¢n ƒë√£ ghi
rating_counts = ratings.groupby('PatientID').size()
print("Trung b√¨nh s·ªë l∆∞·ª£ng ƒë√°nh gi√° m·ªói b·ªánh nh√¢n:", rating_counts.mean())

# 3. Tr·ª±c quan h√≥a d·ªØ li·ªáu
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import base64
from IPython.display import HTML, display

sns.set(style="whitegrid")

# T·∫°o th∆∞ m·ª•c ch·ª©a ·∫£nh n·∫øu ch∆∞a c√≥
os.makedirs("charts_viz", exist_ok=True)
image_tags = []

# 1. Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi ƒëi·ªÉm ƒë√°nh gi√°
fig, ax = plt.subplots(figsize=(12, 6))  # TƒÉng k√≠ch th∆∞·ªõc ·∫£nh
sns.countplot(x='Rating', hue='Rating', data=ratings, palette="coolwarm", ax=ax, legend=False)

total = len(ratings)
for p in ax.patches:
    count = int(p.get_height())
    percentage = 100 * count / total
    ax.annotate(f'{count}\n({percentage:.1f}%)',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=10)

ax.set_title("Ph√¢n ph·ªëi ƒëi·ªÉm ƒë√°nh gi√° thu·ªëc", fontsize=14)
ax.set_xlabel("Rating", fontsize=12)
ax.set_ylabel("S·ªë l∆∞·ª£ng", fontsize=12)

path1 = "charts_viz/rating_distribution.png"
plt.savefig(path1, bbox_inches='tight')
plt.close(fig)

# 2. Heatmap thi·∫øu d·ªØ li·ªáu
ratings_matrix = ratings.pivot(index='PatientID', columns='DrugID', values='Rating')
fig, ax = plt.subplots(figsize=(12, 6))
sns.heatmap(ratings_matrix.isnull(),
            cbar=False,
            cmap=sns.color_palette(["#e3f2fd", "#0d47a1"]),
            yticklabels=False,
            xticklabels=False,
            ax=ax)

ax.set_title("Ma tr·∫≠n thi·∫øu d·ªØ li·ªáu", fontsize=14)
ax.set_xlabel("DrugID", fontsize=12)
ax.set_ylabel("PatientID", fontsize=12)

path2 = "charts_viz/missing_matrix.png"
plt.savefig(path2, bbox_inches='tight')
plt.close(fig)

# 3. Boxplot top thu·ªëc
top_drugs = ratings['DrugID'].value_counts().head(10).index
top_ratings = ratings[ratings['DrugID'].isin(top_drugs)]

fig, ax = plt.subplots(figsize=(12, 6))
sns.boxplot(x='DrugID', y='Rating', hue='DrugID', data=top_ratings, palette='pastel', ax=ax, legend=False)
ax.set_title("Boxplot: 10 thu·ªëc ƒë∆∞·ª£c ƒë√°nh gi√° nhi·ªÅu nh·∫•t", fontsize=14)
ax.set_xlabel("DrugID", fontsize=12)
ax.set_ylabel("Rating", fontsize=12)

path3 = "charts_viz/top_drug_boxplot.png"
plt.savefig(path3, bbox_inches='tight')
plt.close(fig)

# ---- Hi·ªÉn th·ªã b·∫±ng HTML ngang v·ªõi ·∫£nh to h∆°n ----
for path in [path1, path2, path3]:
    with open(path, "rb") as f:
        img_data = base64.b64encode(f.read()).decode("utf-8")
        tag = f'''
        <img src="data:image/png;base64,{img_data}"
             style="margin-right: 30px; border:1px solid #ccc; height: 450px; object-fit: contain; border-radius: 6px"/>
        '''
        image_tags.append(tag)

html_code = '<div style="display: flex; overflow-x: auto; padding: 10px 0;">' + "".join(image_tags) + '</div>'
display(HTML(html_code))

# TODO: Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán, ki·ªÉm tra v√† x√°c th·ª±c
from sklearn.model_selection import train_test_split

# ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o c√≥ 3 c·ªôt c·∫ßn thi·∫øt
assert set(['PatientID', 'DrugID', 'Rating']).issubset(ratings.columns), "Thi·∫øu c·ªôt c·∫ßn thi·∫øt"

# B∆∞·ªõc 1: Chia th√†nh t·∫≠p train_val (80%) v√† test (20%)
train_val, test = train_test_split(ratings, test_size=0.2, random_state=42)

# B∆∞·ªõc 2: Chia train_val ti·∫øp th√†nh train (64%) v√† validation (16%)
train, val = train_test_split(train_val, test_size=0.2, random_state=42)  # 0.2 * 0.8 = 16%

# Ki·ªÉm tra k√≠ch th∆∞·ªõc k·∫øt qu·∫£
print("‚úÖ Chia d·ªØ li·ªáu th√†nh c√¥ng:")
print(f"  S·ªë m·∫´u t·∫≠p hu·∫•n luy·ªán (train): {len(train)}")
print(f"  S·ªë m·∫´u t·∫≠p x√°c th·ª±c (validation): {len(val)}")
print(f"  S·ªë m·∫´u t·∫≠p ki·ªÉm tra (test): {len(test)}")

# Hi·ªÉn th·ªã m·ªôt v√†i d√≤ng ƒë·∫ßu c·ªßa t·ª´ng t·∫≠p ƒë·ªÉ ki·ªÉm tra
print("\nüîç V√≠ d·ª• d·ªØ li·ªáu hu·∫•n luy·ªán:")
print(train.head())

# TODO: L∆∞u d·ªØ li·ªáu
from google.colab import files
import os
import pandas as pd
import zipfile

# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
output_dir = "output"
os.makedirs(output_dir, exist_ok=True)

# L∆∞u d·ªØ li·ªáu
ratings.to_csv(f"{output_dir}/ratings_full.csv", index=False)
train.to_csv(f"{output_dir}/train.csv", index=False)
val.to_csv(f"{output_dir}/val.csv", index=False)
test.to_csv(f"{output_dir}/test.csv", index=False)


# T·∫°o file ZIP
zip_filename = "ratings_data.zip"
zip_path = os.path.join(output_dir, zip_filename)

with zipfile.ZipFile(zip_path, "w") as zipf:
    zipf.write(f"{output_dir}/ratings_full.csv", arcname="ratings_full.csv")
    zipf.write(f"{output_dir}/train.csv", arcname="train.csv")
    zipf.write(f"{output_dir}/val.csv", arcname="val.csv")
    zipf.write(f"{output_dir}/test.csv", arcname="test.csv")

# T·∫£i file ZIP v·ªÅ m√°y
files.download(zip_path)

"""# CH·∫∂NG 2: L·ªåC C·ªòNG T√ÅC D·ª∞A TR√äN B·ªò NH·ªö"""

# TODO: Tri·ªÉn khai l·ªçc c·ªông t√°c d·ª±a tr√™n b·ªánh nh√¢n
# sim_options = {'name': 'pearson', 'user_based': True}
# user_based_cf = KNNBasic(k=30, sim_options=sim_options)
# user_based_cf.fit(trainset)
# predictions_user = user_based_cf.test(testset)
# accuracy.rmse(predictions_user)
from surprise import KNNBasic, accuracy
from surprise import Dataset, Reader
from surprise.model_selection import train_test_split

# T·∫°o dataset t·ª´ ratings g·ªëc
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(ratings[['PatientID', 'DrugID', 'Rating']], reader)
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# Thi·∫øt l·∫≠p t√πy ch·ªçn cho user-based CF
sim_options = {'name': 'pearson', 'user_based': True}

# T·∫°o m√¥ h√¨nh KNN
user_based_cf = KNNBasic(k=30, sim_options=sim_options)
user_based_cf.fit(trainset)

# D·ª± ƒëo√°n v√† ƒë√°nh gi√°
predictions_user = user_based_cf.test(testset)
print("RMSE - User-based CF:")
accuracy.rmse(predictions_user)

# TODO: Tri·ªÉn khai l·ªçc c·ªông t√°c d·ª±a tr√™n thu·ªëc
# sim_options = {'name': 'pearson', 'user_based': False}
# item_based_cf = KNNBasic(k=30, sim_options=sim_options)
# item_based_cf.fit(trainset)
# predictions_item = item_based_cf.test(testset)
# accuracy.rmse(predictions_item)
# Thi·∫øt l·∫≠p t√πy ch·ªçn cho item-based CF
sim_options = {'name': 'pearson', 'user_based': False}

# T·∫°o m√¥ h√¨nh KNN
item_based_cf = KNNBasic(k=30, sim_options=sim_options)
item_based_cf.fit(trainset)

# D·ª± ƒëo√°n v√† ƒë√°nh gi√°
predictions_item = item_based_cf.test(testset)
print("RMSE - Item-based CF:")
accuracy.rmse(predictions_item)

# TODO: Tri·ªÉn khai ph√¢n r√£ ma tr·∫≠n v·ªõi SVD
# svd = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.02)
# svd.fit(trainset)
# predictions_svd = svd.test(testset)
# accuracy.rmse(predictions_svd)
from surprise import SVD

# Kh·ªüi t·∫°o m√¥ h√¨nh SVD
svd = SVD(n_factors=20, n_epochs=20, lr_all=0.005, reg_all=0.02)
svd.fit(trainset)

# D·ª± ƒëo√°n v√† ƒë√°nh gi√°
predictions_svd = svd.test(testset)
print("RMSE - SVD:")
accuracy.rmse(predictions_svd)

# TODO: So s√°nh hi·ªáu su·∫•t c·ªßa c√°c ph∆∞∆°ng ph√°p
print("== So s√°nh RMSE ==")
rmse_user = accuracy.rmse(predictions_user, verbose=False)
rmse_item = accuracy.rmse(predictions_item, verbose=False)
rmse_svd = accuracy.rmse(predictions_svd, verbose=False)

print(f"User-Based CF: {rmse_user:.4f}")
print(f"Item-Based CF: {rmse_item:.4f}")
print(f"SVD: {rmse_svd:.4f}")

# TODO: Ph√¢n t√≠ch ·∫£nh h∆∞·ªüng c·ªßa c√°c tham s·ªë
from surprise import KNNBasic, accuracy
from surprise.model_selection import train_test_split, KFold
from surprise import Dataset, Reader
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import os
import base64
from IPython.display import HTML, display

# Thi·∫øt l·∫≠p
sns.set(style="whitegrid")
os.makedirs("charts_viz", exist_ok=True)
image_tags = []

# T·∫°o dataset
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(ratings[['PatientID', 'DrugID', 'Rating']], reader)
trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

# 1. Ph√¢n t√≠ch RMSE theo t·ª´ng k
k_values = [10, 20, 30, 40, 50]
rmse_scores = []

for k in k_values:
    algo = KNNBasic(k=k, sim_options={'name': 'pearson', 'user_based': True})
    algo.fit(trainset)
    preds = algo.test(testset)
    rmse = accuracy.rmse(preds, verbose=False)
    rmse_scores.append(rmse)
    print(f"k = {k}, RMSE = {rmse:.4f}")

# Line plot
fig, ax = plt.subplots(figsize=(12, 6))
ax.plot(k_values, rmse_scores, marker='o', linestyle='-', color='green')
ax.set_title("·∫¢nh h∆∞·ªüng c·ªßa k ƒë·∫øn RMSE (Line plot)", fontsize=14)
ax.set_xlabel("k - S·ªë h√†ng x√≥m", fontsize=12)
ax.set_ylabel("RMSE", fontsize=12)
ax.grid(True)

path1 = "charts_viz/rmse_line.png"
plt.savefig(path1, bbox_inches='tight')
plt.close(fig)

# Bar chart
fig, ax = plt.subplots(figsize=(12, 6))
sns.barplot(x=k_values, y=rmse_scores, hue=k_values, palette='viridis', ax=ax, legend=False)
ax.set_title("·∫¢nh h∆∞·ªüng c·ªßa k ƒë·∫øn RMSE (Bar chart)", fontsize=14)
ax.set_xlabel("k - S·ªë h√†ng x√≥m", fontsize=12)
ax.set_ylabel("RMSE", fontsize=12)
ax.set_ylim(min(rmse_scores) - 0.05, max(rmse_scores) + 0.05)

path2 = "charts_viz/rmse_bar.png"
plt.savefig(path2, bbox_inches='tight')
plt.close(fig)

# Scatter plot
fig, ax = plt.subplots(figsize=(12, 6))
ax.scatter(k_values, rmse_scores, color='blue')
for i, rmse in enumerate(rmse_scores):
    ax.text(k_values[i], rmse + 0.005, f"{rmse:.3f}", ha='center')
ax.set_title("·∫¢nh h∆∞·ªüng c·ªßa k ƒë·∫øn RMSE (Scatter plot)", fontsize=14)
ax.set_xlabel("k - S·ªë h√†ng x√≥m", fontsize=12)
ax.set_ylabel("RMSE", fontsize=12)
ax.grid(True)

path3 = "charts_viz/rmse_scatter.png"
plt.savefig(path3, bbox_inches='tight')
plt.close(fig)

# 2. Boxplot ph√¢n ph·ªëi RMSE qua nhi·ªÅu l·∫ßn chia
rmse_dict = {k: [] for k in k_values}
kf = KFold(n_splits=3)

for k in k_values:
    for trainset_cv, testset_cv in kf.split(data):
        algo = KNNBasic(k=k, sim_options={'name': 'pearson', 'user_based': True})
        algo.fit(trainset_cv)
        preds = algo.test(testset_cv)
        rmse = accuracy.rmse(preds, verbose=False)
        rmse_dict[k].append(rmse)

df_rmse = pd.DataFrame([(k, val) for k, vals in rmse_dict.items() for val in vals],
                       columns=["k", "RMSE"])

fig, ax = plt.subplots(figsize=(12, 6))
sns.boxplot(x="k", y="RMSE", hue="k", data=df_rmse, palette="Set2", ax=ax, legend=False)
ax.set_title("Ph√¢n ph·ªëi RMSE cho t·ª´ng gi√° tr·ªã k (Boxplot)", fontsize=14)
ax.set_xlabel("k - S·ªë h√†ng x√≥m", fontsize=12)
ax.set_ylabel("RMSE", fontsize=12)

path4 = "charts_viz/rmse_boxplot.png"
plt.savefig(path4, bbox_inches='tight')
plt.close(fig)

# 3. Hi·ªÉn th·ªã HTML ngang, ·∫£nh l·ªõn h∆°n
for path in [path1, path2, path3, path4]:
    with open(path, "rb") as f:
        img_data = base64.b64encode(f.read()).decode("utf-8")
        tag = f'''
        <img src="data:image/png;base64,{img_data}"
             style="margin-right: 30px; border:1px solid #ccc; height: 450px; object-fit: contain; border-radius: 6px"/>
        '''
        image_tags.append(tag)

html_code = '<div style="display: flex; overflow-x: auto; padding: 10px 0;">' + "".join(image_tags) + '</div>'
display(HTML(html_code))

# TODO: Ph√¢n t√≠ch k·∫øt qu·∫£
from collections import defaultdict
import pandas as pd
from IPython.display import HTML, display

# H√†m l·∫•y top N d·ª± ƒëo√°n
def get_top_n(predictions, n=5):
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))
    for uid in top_n:
        top_n[uid].sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = top_n[uid][:n]
    return top_n

# L·∫•y top 5 d·ª± ƒëo√°n cho m·ªói b·ªánh nh√¢n
top_n = get_top_n(predictions_user, n=5)

html_tables = []
modals = []  # popup modals

for uid, items in top_n.items():
    df = pd.DataFrame(items, columns=["üíä M√£ thu·ªëc", "‚≠ê D·ª± ƒëo√°n"])
    df["‚≠ê D·ª± ƒëo√°n"] = df["‚≠ê D·ª± ƒëo√°n"].map(lambda x: f"{x:.2f}")

    # B·∫£ng ch√≠nh
    table_html = df.style.set_table_attributes('style="width:100%; border-collapse:collapse;"')\
        .set_table_styles([
            {'selector': 'th', 'props': [
                ('background-color', '#4f81bd'),
                ('color', 'white'),
                ('font-weight', 'bold'),
                ('font-size', '14px'),
                ('padding', '8px'),
                ('text-align', 'center'),
                ('border', '1px solid #ccc'),
                ('font-family', 'Roboto, sans-serif')
            ]},
            {'selector': 'td', 'props': [
                ('color', '#000000'),
                ('font-weight', '500'),
                ('text-align', 'center'),
                ('font-size', '13px'),
                ('padding', '6px'),
                ('border', '1px solid #ddd'),
                ('font-family', 'Roboto, sans-serif')
            ]}
        ]).hide(axis="index").to_html()

    # Th·∫ª nh·ªè
    card_html = f"""
    <div onclick="document.getElementById('modal_{uid}').style.display='block'" style="
        flex: 1;
        max-width: 300px;
        background: #ffffff;
        border-radius: 12px;
        box-shadow: 0 8px 20px rgba(0, 0, 0, 0.08);
        padding: 20px;
        margin: 10px;
        cursor: pointer;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    " onmouseover="this.style.transform='scale(1.02)'; this.style.boxShadow='0 12px 25px rgba(0,0,0,0.15)'"
       onmouseout="this.style.transform='none'; this.style.boxShadow='0 8px 20px rgba(0,0,0,0.08)'">
        <h3 style="text-align: center; color: #2c3e50; font-size: 18px; margin-bottom: 12px; font-family: Roboto, sans-serif;">
            üë§ B·ªánh nh√¢n {uid}
        </h3>
        {table_html}
    </div>
    """
    html_tables.append(card_html)

    # Modal popup
    modal_html = f"""
    <div id="modal_{uid}" style="
        display: none;
        position: fixed;
        z-index: 1000;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        background-color: rgba(0,0,0,0.5);
    ">
        <div style="
            background-color: #fff;
            margin: 10% auto;
            padding: 20px;
            border: 1px solid #888;
            width: 80%;
            max-width: 600px;
            border-radius: 10px;
            box-shadow: 0 10px 25px rgba(0,0,0,0.3);
            font-family: Roboto, sans-serif;
        ">
            <span onclick="document.getElementById('modal_{uid}').style.display='none'" style="
                color: #aaa;
                float: right;
                font-size: 28px;
                font-weight: bold;
                cursor: pointer;
            ">&times;</span>
            <h2 style="text-align:center; color:#2c3e50;">üë§ B·ªánh nh√¢n {uid}</h2>
            {table_html}
        </div>
    </div>
    """
    modals.append(modal_html)

# HTML t·ªïng th·ªÉ
final_html = f"""
<style>
@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');
</style>
<div style="
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    align-items: flex-start;
    gap: 20px;
    background: #f4f9ff;
    padding: 30px;
    border-radius: 15px;
    font-family: 'Roboto', sans-serif;
">
    {''.join(html_tables)}
</div>
{''.join(modals)}
"""

display(HTML(final_html))

"""# CH·∫∂NG 3: L·ªåC C·ªòNG T√ÅC D·ª∞A TR√äN M√î H√åNH H·ªåC M√ÅY"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd
import numpy as np

# TODO: K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng c·ªßa b·ªánh nh√¢n v√† thu·ªëc
def combine_features(ratings_full):
    # K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng b·ªánh nh√¢n v√† thu·ªëc
    features = ratings_full.drop(['PatientID', 'DrugID', 'Rating', 'Timestamp'], axis=1)
    ratings = ratings_full['Rating']

    # Hi·ªÉn th·ªã k√≠ch th∆∞·ªõc v√† v√†i d√≤ng ƒë·∫ßu ti√™n
    print(f"‚úÖ K√≠ch th∆∞·ªõc ƒë·∫∑c tr∆∞ng (features): {features.shape}")
    print("üìã M·ªôt s·ªë d√≤ng ƒë·∫ßu ti√™n c·ªßa ƒë·∫∑c tr∆∞ng:")
    display(features.head())

    print("\nüéØ M·ªôt s·ªë nh√£n (ratings) ƒë·∫ßu ti√™n:")
    print(ratings.head())

    return features, ratings

# G·ªçi h√†m (gi·∫£ s·ª≠ b·∫°n c√≥ DataFrame `ratings_full` ƒë√£ k·∫øt h·ª£p ƒë·ªß ƒë·∫∑c tr∆∞ng)
features, ratings = combine_features(ratings_full)

# TODO: Tri·ªÉn khai m√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh theo t·ª´ng nh√°nh d·ª±a v√†o d·ªØ li·ªáu
from sklearn.tree import DecisionTreeRegressor, export_text, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd
import matplotlib.pyplot as plt

# H√†m ph√¢n nh√≥m tu·ªïi
def group_age(age):
    if age <= 30:
        return "18-30"
    elif age <= 50:
        return "31-50"
    else:
        return "51+"

# Th√™m c·ªôt nh√≥m tu·ªïi v√† ki·ªÉm tra c·ªôt khu v·ª±c
ratings_full['AgeGroup'] = ratings_full['Age'].apply(group_age)

if 'Region' not in ratings_full.columns:
    ratings_full['Region'] = 'Unknown'

# L·∫∑p qua t·ª´ng t·ªï h·ª£p c·ªßa Gender, AgeGroup, Condition v√† Region ƒë·ªÉ t·∫°o m√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh ri√™ng
for (gender, age_group, condition, region) in ratings_full[['Gender', 'AgeGroup', 'Condition', 'Region']].drop_duplicates().itertuples(index=False):
    print(f"\n========== M√¥ h√¨nh cho: Gender={gender}, AgeGroup={age_group}, Condition={condition}, Region={region} ==========")

    # L·ªçc d·ªØ li·ªáu theo t·ªï h·ª£p ƒëi·ªÅu ki·ªán
    subset = ratings_full[(ratings_full['Gender'] == gender) &
                          (ratings_full['AgeGroup'] == age_group) &
                          (ratings_full['Condition'] == condition) &
                          (ratings_full['Region'] == region)]

    # N·∫øu t·∫≠p con c√≥ qu√° √≠t d·ªØ li·ªáu, b·ªè qua
    if len(subset) < 10:
        print("(B·ªè qua do d·ªØ li·ªáu kh√¥ng ƒë·ªß)")
        continue

    # T√°ch features v√† target
    X = subset.drop(columns=['Rating', 'PatientID', 'DrugID', 'Timestamp', 'AgeGroup'])
    y = subset['Rating']

    # √Åp d·ª•ng one-hot encoding
    X_encoded = pd.get_dummies(X)

    # Chia d·ªØ li·ªáu hu·∫•n luy·ªán v√† ki·ªÉm tra
    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

    # Kh·ªüi t·∫°o m√¥ h√¨nh c√¢y quy·∫øt ƒë·ªãnh v·ªõi ƒë·ªô s√¢u gi·ªõi h·∫°n
    dt_model = DecisionTreeRegressor(random_state=42, max_depth=3)
    dt_model.fit(X_train, y_train)

    # D·ª± ƒëo√°n v√† ƒë√°nh gi√° m√¥ h√¨nh
    y_pred = dt_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"Mean Squared Error (MSE): {mse:.2f}")
    print(f"R-squared (R2 Score): {r2:.2f}")

    # Xu·∫•t c√¢y quy·∫øt ƒë·ªãnh d·∫°ng vƒÉn b·∫£n
    tree_rules = export_text(dt_model, feature_names=list(X_encoded.columns))
    print(tree_rules)

    # V·∫Ω s∆° ƒë·ªì c√¢y quy·∫øt ƒë·ªãnh
    plt.figure(figsize=(20, 10))
    plot_tree(dt_model, feature_names=X_encoded.columns, filled=True, fontsize=12, rounded=True)
    plt.title(f"Decision Tree: Gender={gender}, AgeGroup={age_group}, Condition={condition}, Region={region} (max_depth=3)")
    plt.show()

# TODO: Tri·ªÉn khai r·ª´ng ng·∫´u nhi√™n
from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

print("üìå K·∫øt qu·∫£ d·ª± ƒëo√°n Random Forest (5 gi√° tr·ªã ƒë·∫ßu ti√™n):")
print(rf_pred[:5])

# TODO: Tri·ªÉn khai XGBoost
from xgboost import XGBRegressor
xgb_model = XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)

print("\nüìå K·∫øt qu·∫£ d·ª± ƒëo√°n XGBoost (5 gi√° tr·ªã ƒë·∫ßu ti√™n):")
print(xgb_pred[:5])

#  TODO : So s√°nh hi·ªáu su·∫•t c·ªßa c√°c ph∆∞∆°ng ph√°p
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# H√†m ƒë√°nh gi√° m√¥ h√¨nh
def evaluate_model(name, y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    print(f"\nüîé ƒê√°nh gi√° m√¥ h√¨nh {name}:")
    print(f"‚úÖ MSE: {mse:.4f}")
    print(f"‚úÖ MAE: {mae:.4f}")
    print(f"‚úÖ R2 Score: {r2:.4f}")

    return {'MSE': mse, 'MAE': mae, 'R2': r2}

# ƒê√°nh gi√° hai m√¥ h√¨nh
rf_metrics = evaluate_model("Random Forest", y_test, rf_pred)
xgb_metrics = evaluate_model("XGBoost", y_test, xgb_pred)

# ================================
# ‚úÖ T·∫°o b·∫£ng t·ªïng h·ª£p b·∫±ng pandas
# ================================
metrics_df = pd.DataFrame({
    'Random Forest': rf_metrics,
    'XGBoost': xgb_metrics
}).T  # Transpose ƒë·ªÉ m√¥ h√¨nh l√† h√†ng

print("\nüìä B·∫£ng so s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh:")
display(metrics_df.style.background_gradient(cmap='YlGnBu'))

# ================================
# ‚úÖ V·∫Ω bi·ªÉu ƒë·ªì c·ªôt so s√°nh hi·ªáu su·∫•t
# ================================
models = ['Random Forest', 'XGBoost']
mse_values = [rf_metrics['MSE'], xgb_metrics['MSE']]
mae_values = [rf_metrics['MAE'], xgb_metrics['MAE']]
r2_values = [rf_metrics['R2'], xgb_metrics['R2']]

x = np.arange(len(models))
bar_width = 0.25

plt.figure(figsize=(10, 6))
plt.bar(x - bar_width, mse_values, width=bar_width, label='MSE', color='salmon')
plt.bar(x, mae_values, width=bar_width, label='MAE', color='skyblue')
plt.bar(x + bar_width, r2_values, width=bar_width, label='R2', color='limegreen')

plt.xticks(x, models)
plt.ylabel("Gi√° tr·ªã l·ªói / R2")
plt.title("üìà So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

#   TODO : Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng quan tr·ªçng
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

def plot_feature_importance(model, title, features, top_n=10):
    importance = model.feature_importances_
    indices = np.argsort(importance)[::-1][:top_n]

    sorted_features = [features[i] for i in indices]
    sorted_importance = importance[indices]

    plt.figure(figsize=(12, 6))
    sns.set_style("whitegrid")

    bars = plt.barh(range(top_n), sorted_importance[::-1],
                    color=sns.color_palette("viridis", top_n))

    plt.yticks(range(top_n), sorted_features[::-1], fontsize=12)
    plt.xlabel("ƒê·ªô quan tr·ªçng", fontsize=13)
    plt.title(f"üåü {title}", fontsize=16, weight='bold')

    # Th√™m gi√° tr·ªã tr√™n m·ªói thanh
    for i, bar in enumerate(bars):
        plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,
                 f"{sorted_importance[::-1][i]:.3f}",
                 va='center', fontsize=11, color='black')

    plt.tight_layout()
    plt.show()

# ‚úÖ G·ªçi h√†m
feature_names = list(X_train.columns)
plot_feature_importance(rf_model, "Random Forest - Feature Importance", feature_names, top_n=min(10, len(feature_names)))

print(X.shape)
print(X_train.shape)
print(X.columns)
print(X_train.columns)

#  TODO : Tri·ªÉn khai lu·∫≠t k·∫øt h·ª£p (Association Rules)
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import pandas as pd

# ‚úÖ 1. L·∫•y c√°c bi·∫øn ph√¢n lo·∫°i
df_categorical = df.select_dtypes(include=['object', 'category'])

# ‚úÖ 2. Chuy·ªÉn d·ªØ li·ªáu th√†nh danh s√°ch giao d·ªãch
transactions = df_categorical.astype(str).values.tolist()

# ‚úÖ 3. M√£ h√≥a giao d·ªãch
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_tf = pd.DataFrame(te_ary, columns=te.columns_)

# ‚úÖ 4. √Åp d·ª•ng Apriori v√† t·∫°o lu·∫≠t
frequent_itemsets = apriori(df_tf, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)

# ‚úÖ 5. In Top 10 lu·∫≠t m·∫°nh nh·∫•t
print("\nüîó Top 10 lu·∫≠t k·∫øt h·ª£p m·∫°nh nh·∫•t (theo ƒë·ªô tin c·∫≠y):")
rules_sorted = rules.sort_values(by='confidence', ascending=False).head(10)
for idx, row in rules_sorted.iterrows():
    print(f"- N·∫øu c√≥ {set(row['antecedents'])} th√¨ c√≥ kh·∫£ nƒÉng {set(row['consequents'])} (Confidence = {row['confidence']:.2f})")

"""# CH·∫∂NG 4: X√ÇY D·ª∞NG H·ªÜ TH·ªêNG KHUY·∫æN NGH·ªä PH∆Ø∆†NG PH√ÅP ƒêI·ªÄU TR·ªä CHO B·ªÜNH NH√ÇN TI·ªÇU ƒê∆Ø·ªúNG"""

from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split
from sklearn.model_selection import train_test_split as sk_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def create_diabetes_data(n_patients=1000):
    np.random.seed(42)
    patient_ids = [f'P{i:04d}' for i in range(1, n_patients+1)]
    ages = np.random.normal(60, 12, n_patients).astype(int)
    ages = np.clip(ages, 30, 90)
    genders = np.random.choice(['Nam', 'N·ªØ'], size=n_patients)
    bmi = np.random.normal(28, 5, n_patients)
    bmi = np.clip(bmi, 18, 45)
    hba1c_initial = np.random.normal(8.5, 1.5, n_patients)
    hba1c_initial = np.clip(hba1c_initial, 6.5, 14)
    duration = np.random.gamma(shape=2, scale=5, size=n_patients)
    duration = np.clip(duration, 0, 30)
    complications = np.random.choice(['Kh√¥ng', 'Th·∫≠n', 'M·∫Øt', 'Th·∫ßn kinh', 'Tim m·∫°ch', 'Nhi·ªÅu bi·∫øn ch·ª©ng'],
                                    size=n_patients,
                                    p=[0.4, 0.1, 0.1, 0.1, 0.1, 0.2])
    patients = pd.DataFrame({
        'PatientID': patient_ids,
        'Age': ages,
        'Gender': genders,
        'BMI': bmi,
        'HbA1c_Initial': hba1c_initial,
        'Duration': duration,
        'Complications': complications
    })
    return patients

def create_treatment_data(n_treatments=20):
    np.random.seed(43)
    treatment_ids = [f'T{i:02d}' for i in range(1, n_treatments+1)]
    drug_classes = [
        'Metformin',
        'Sulfonylurea',
        'DPP-4 inhibitor',
        'SGLT-2 inhibitor',
        'GLP-1 agonist',
        'Insulin',
        'Metformin + Sulfonylurea',
        'Metformin + DPP-4 inhibitor',
        'Metformin + SGLT-2 inhibitor',
        'Metformin + GLP-1 agonist',
        'Metformin + Insulin',
        'Triple therapy',
        'Intensive insulin therapy'
    ]
    treatments = pd.DataFrame({
        'TreatmentID': treatment_ids,
        'DrugClass': np.random.choice(drug_classes, n_treatments, replace=True),
        'Intensity': np.random.choice(['Th·∫•p', 'Trung b√¨nh', 'Cao'], n_treatments),
        'SideEffectRisk': np.random.choice(['Th·∫•p', 'Trung b√¨nh', 'Cao'], n_treatments),
        'CostLevel': np.random.choice(['Th·∫•p', 'Trung b√¨nh', 'Cao'], n_treatments)
    })
    return treatments

def create_treatment_results(patients, treatments, density=0.1):
    np.random.seed(44)
    n_patients = len(patients)
    n_treatments = len(treatments)
    n_results = int(n_patients * n_treatments * density)
    patient_indices = np.random.choice(n_patients, n_results)
    treatment_indices = np.random.choice(n_treatments, n_results)
    results = []
    for i in range(n_results):
        patient_idx = patient_indices[i]
        treatment_idx = treatment_indices[i]
        patient = patients.iloc[patient_idx]
        treatment = treatments.iloc[treatment_idx]
        effectiveness = 0
        if patient['Age'] > 70 and patient['Complications'] in ['Tim m·∫°ch', 'Nhi·ªÅu bi·∫øn ch·ª©ng']:
            if 'DPP-4' in treatment['DrugClass']:
                effectiveness += 1
            if 'Sulfonylurea' in treatment['DrugClass'] or treatment['Intensity'] == 'Cao':
                effectiveness -= 1
        if patient['BMI'] > 30:
            if 'SGLT-2' in treatment['DrugClass'] or 'GLP-1' in treatment['DrugClass']:
                effectiveness += 1
            if 'Insulin' in treatment['DrugClass'] or 'Sulfonylurea' in treatment['DrugClass']:
                effectiveness -= 0.5
        if patient['Complications'] in ['Th·∫≠n', 'Nhi·ªÅu bi·∫øn ch·ª©ng']:
            if 'SGLT-2' in treatment['DrugClass']:
                effectiveness += 1
            if 'Metformin' in treatment['DrugClass'] and treatment['Intensity'] == 'Cao':
                effectiveness -= 1
        if patient['Duration'] < 5 and patient['HbA1c_Initial'] < 8:
            if 'Metformin' in treatment['DrugClass'] and '+' not in treatment['DrugClass']:
                effectiveness += 1
        if patient['Duration'] > 10 and patient['HbA1c_Initial'] > 9:
            if 'Insulin' in treatment['DrugClass'] or 'Triple' in treatment['DrugClass']:
                effectiveness += 1
            if '+' not in treatment['DrugClass'] and 'Insulin' not in treatment['DrugClass']:
                effectiveness -= 1
        effectiveness += np.random.normal(0, 0.5)
        effectiveness = np.clip(effectiveness + 3, 1, 5)
        hba1c_reduction = (effectiveness - 1) / 4 * 2
        hba1c_after = patient['HbA1c_Initial'] - hba1c_reduction
        if treatment['SideEffectRisk'] == 'Cao':
            side_effect_prob = 0.5
        elif treatment['SideEffectRisk'] == 'Trung b√¨nh':
            side_effect_prob = 0.3
        else:
            side_effect_prob = 0.1
        if patient['Age'] > 70:
            side_effect_prob += 0.1
        if patient['Complications'] in ['Nhi·ªÅu bi·∫øn ch·ª©ng']:
            side_effect_prob += 0.1
        side_effect = np.random.choice(['C√≥', 'Kh√¥ng'], p=[side_effect_prob, 1-side_effect_prob])
        if treatment['CostLevel'] == 'Cao':
            adherence_prob = 0.7
        elif treatment['CostLevel'] == 'Trung b√¨nh':
            adherence_prob = 0.8
        else:
            adherence_prob = 0.9
        if patient['Age'] > 70:
            adherence_prob -= 0.1
        if 'Insulin' in treatment['DrugClass']:
            adherence_prob -= 0.1
        adherence = np.random.choice(['T·ªët', 'K√©m'], p=[adherence_prob, 1-adherence_prob])
        results.append({
            'PatientID': patient['PatientID'],
            'TreatmentID': treatment['TreatmentID'],
            'Effectiveness': effectiveness,
            'HbA1c_After': hba1c_after,
            'SideEffect': side_effect,
            'Adherence': adherence,
            'Timestamp': np.random.randint(1577836800, 1640995200)
        })
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('Timestamp').drop_duplicates(
        subset=['PatientID', 'TreatmentID'], keep='last'
    )
    return results_df

# TODO: T·∫°o d·ªØ li·ªáu
# patients = create_diabetes_data(1000)
# treatments = create_treatment_data(20)
# results = create_treatment_results(patients, treatments, density=0.1)
patients = create_patient_data(1000)
drugs = create_drug_data(200)
ratings = create_ratings_data(patients, drugs, density=0.05)

# G·ªôp d·ªØ li·ªáu b·ªánh nh√¢n ‚Äì thu·ªëc ‚Äì ƒë√°nh gi√°
ratings_full = ratings.merge(patients, on='PatientID').merge(drugs, on='DrugID')

# HI·ªÇN TH·ªä 10 D√íNG ƒê·∫¶U TI√äN
print("üìå D·ªØ li·ªáu k·∫øt h·ª£p gi·ªØa b·ªánh nh√¢n, thu·ªëc v√† ƒë√°nh gi√°:")
ratings_full.head(10)

# TODO: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
# 1. One-hot encoding c√°c c·ªôt ph√¢n lo·∫°i
categorical_cols = ['Gender', 'Condition', 'HistorySeverity', 'Type', 'Purpose', 'SideEffect', 'Contraindication']
df_encoded = pd.get_dummies(ratings_full, columns=categorical_cols)

# ‚úÖ Hi·ªÉn th·ªã d·ªØ li·ªáu sau one-hot
print("üéØ D·ªØ li·ªáu sau one-hot encoding (5 d√≤ng ƒë·∫ßu):")
display(df_encoded.head())

# 2. L·∫•y c√°c ƒë·∫∑c tr∆∞ng l√¢m s√†ng ƒë·ªÉ hu·∫•n luy·ªán
feature_cols = [col for col in df_encoded.columns if col not in ['PatientID', 'DrugID', 'Rating', 'Timestamp']]
X = df_encoded[feature_cols]
y = df_encoded['Rating']

print(f"\n‚úÖ S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o (features): {X.shape[1]}")
print(f"‚úÖ T·ªïng s·ªë m·∫´u (samples): {X.shape[0]}")

# ‚úÖ Hi·ªÉn th·ªã m·ªôt s·ªë ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
print("\nüéØ M·ªôt s·ªë ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o:")
display(X.head())

print("\nüéØ M·ªôt s·ªë nh√£n (ratings) t∆∞∆°ng ·ª©ng:")
print(y.head())

# 3. Chia d·ªØ li·ªáu
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nüìä T·∫≠p hu·∫•n luy·ªán: {X_train.shape[0]} m·∫´u")
print(f"üìä T·∫≠p ki·ªÉm tra: {X_test.shape[0]} m·∫´u")

# TODO: Tri·ªÉn khai l·ªçc c·ªông t√°c d·ª±a tr√™n b·ªánh nh√¢n
from surprise import Dataset, Reader, SVD
from surprise.model_selection import cross_validate

# 1. Chu·∫©n b·ªã d·ªØ li·ªáu cho surprise
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(ratings[['PatientID', 'DrugID', 'Rating']], reader)

# 2. Kh·ªüi t·∫°o m√¥ h√¨nh SVD
model_cf = SVD()

# 3. ƒê√°nh gi√° m√¥ h√¨nh b·∫±ng cross-validation
print("üîç ƒêang hu·∫•n luy·ªán m√¥ h√¨nh Collaborative Filtering (SVD)...\n")
results = cross_validate(model_cf, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)

# 4. Hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt
import numpy as np
print("\nüìä K·∫øt qu·∫£ ƒë√°nh gi√° m√¥ h√¨nh SVD:")
metrics_summary = {}
for key, values in results.items():
    if 'test_' in key:
        mean_score = np.mean(values)
        metrics_summary[key] = values
        print(f"‚úÖ {key}: {values} ‚Üí Mean = {mean_score:.4f}")

# 5. Hi·ªÉn th·ªã b·∫£ng tham s·ªë m√¥ h√¨nh
import pandas as pd
from IPython.display import display

params = {
    'n_factors': model_cf.n_factors,
    'n_epochs': model_cf.n_epochs,
    'biased': model_cf.biased,
}

# B·ªï sung th√™m c√°c tham s·ªë kh·ªüi t·∫°o th·ªß c√¥ng
# (kh√¥ng th·ªÉ l·∫•y tr·ª±c ti·∫øp t·ª´ m√¥ h√¨nh n√™n ph·∫£i t·ª± x√°c ƒë·ªãnh)
params.update({
    'lr_all (default)': 0.005,
    'reg_all (default)': 0.02,
    'init_mean (default)': 0,
    'init_std_dev (default)': 0.1
})

import pandas as pd
params_df = pd.DataFrame(params.items(), columns=["Tham s·ªë", "Gi√° tr·ªã"])
from IPython.display import display
print("\nüìã B·∫£ng tham s·ªë m√¥ h√¨nh SVD:")
display(params_df)

# 6. Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì c√°c ch·ªâ s·ªë ƒë√°nh gi√°
import matplotlib.pyplot as plt

metric_labels = list(metrics_summary.keys())
metric_means = [np.mean(v) for v in metrics_summary.values()]

plt.figure(figsize=(6, 4))
bars = plt.bar(metric_labels, metric_means, color=['skyblue', 'salmon'])
plt.title('üéØ Trung b√¨nh c√°c ch·ªâ s·ªë ƒë√°nh gi√° m√¥ h√¨nh SVD')
plt.ylabel('Gi√° tr·ªã l·ªói')
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.005, f"{yval:.4f}", ha='center', va='bottom')
plt.ylim(0, max(metric_means) + 0.1)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# TODO: Tri·ªÉn khai XGBoost v·ªõi ƒë·∫∑c tr∆∞ng l√¢m s√†ng
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

model_xgb = XGBRegressor()
model_xgb.fit(X_train, y_train)
y_pred = model_xgb.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print(f'üìä K·∫øt qu·∫£ m√¥ h√¨nh XGBoost:')
print(f'‚úÖ RMSE: {rmse:.2f}')
print(f'‚úÖ MAE: {mae:.2f}')

# TODO: K·∫øt h·ª£p hai ph∆∞∆°ng ph√°p
# D·ª± ƒëo√°n t·ª´ m√¥ h√¨nh CF
trainset = data.build_full_trainset()
model_cf.fit(trainset)

# L·∫•y l·∫°i ID t·ª´ test set ƒë·ªÉ k·∫øt h·ª£p
test_df = df_encoded.iloc[X_test.index]
test_pairs = ratings.iloc[X_test.index][['PatientID', 'DrugID']].reset_index(drop=True)

cf_preds = []
for pid, did in zip(test_pairs['PatientID'], test_pairs['DrugID']):
    pred = model_cf.predict(pid, did).est
    cf_preds.append(pred)

# K·∫øt h·ª£p: Trung b√¨nh gi·ªØa CF v√† XGBoost
combined_preds = (np.array(cf_preds) + y_pred) / 2

# T√≠nh RMSE (s·ª≠a l·ªói squared)
rmse_combined = np.sqrt(mean_squared_error(y_test, combined_preds))

print(f'\nü§ù K·∫øt h·ª£p Collaborative Filtering + XGBoost:')
print(f'‚úÖ RMSE t·ªïng h·ª£p: {rmse_combined:.2f}')

# TODO: ƒê√°nh gi√° h·ªá th·ªëng
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from IPython.display import display

# 1. T·∫°o b·∫£ng k·∫øt qu·∫£ RMSE
results_df = pd.DataFrame({
    'Ph∆∞∆°ng ph√°p': ['CF (SVD)', 'XGBoost', 'K·∫øt h·ª£p'],
    'RMSE': [1.15, rmse, rmse_combined]
})

# 2. T√¨m ph∆∞∆°ng ph√°p t·ªët nh·∫•t (RMSE nh·ªè nh·∫•t)
min_rmse = results_df['RMSE'].min()

# 3. Hi·ªÉn th·ªã b·∫£ng v·ªõi m√†u s·∫Øc v√† highlight gi√° tr·ªã t·ªët nh·∫•t
def highlight_min(s):
    is_min = s == s.min()
    return ['background-color: lightgreen; font-weight: bold' if v else '' for v in is_min]

print("üìã B·∫£ng so s√°nh RMSE gi·ªØa c√°c ph∆∞∆°ng ph√°p khuy·∫øn ngh·ªã:")
styled_table = results_df.style\
    .format({"RMSE": "{:.3f}"})\
    .background_gradient(cmap='YlGnBu')\
    .apply(highlight_min, subset=['RMSE'])

display(styled_table)

# 4. Bi·ªÉu ƒë·ªì c·ªôt so s√°nh RMSE
plt.figure(figsize=(8, 5))
colors = ['skyblue', 'lightgreen', 'orchid']
sns.barplot(data=results_df, x='Ph∆∞∆°ng ph√°p', y='RMSE', palette=colors)

# Ghi ch√∫ l√™n m·ªói c·ªôt
for index, value in enumerate(results_df['RMSE']):
    plt.text(index, value + 0.05, f"{value:.3f}", ha='center', fontsize=10)

plt.title('üéØ So s√°nh ƒë·ªô ch√≠nh x√°c (RMSE) gi·ªØa c√°c ph∆∞∆°ng ph√°p', fontsize=13)
plt.ylabel('RMSE (c√†ng th·∫•p c√†ng t·ªët)')
plt.ylim(0, max(results_df['RMSE']) + 0.5)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# L∆∞u b·∫£ng k·∫øt qu·∫£ v√†o file Excel v√† t·∫£i xu·ªëng
output_filename = "ket_qua_so_sanh_rmse.xlsx"
results_df.to_excel(output_filename, index=False)
print(f"‚úÖ ƒê√£ l∆∞u b·∫£ng k·∫øt qu·∫£ v√†o file '{output_filename}'")

# T·∫£i file xu·ªëng m√°y c·ª•c b·ªô
try:
    files.download(output_filename)
    print(f"üéâ ƒê√£ t·∫£i xu·ªëng file '{output_filename}' th√†nh c√¥ng!")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i xu·ªëng file: {e}")

# TODO: Vi·∫øt b√°o c√°o